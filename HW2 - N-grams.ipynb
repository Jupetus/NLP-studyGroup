{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==========================\n",
    "# Just ported from python to notebook\n",
    "#==========================\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.util import ngrams\n",
    "from nltk import word_tokenize\n",
    "from collections import Counter\n",
    "from nltk.collocations import *\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stemmer = PorterStemmer() # SnowballStemmer('english')\n",
    "rTokenizer = RegexpTokenizer('\\w+')\n",
    "\n",
    "def stemMyStrings(words):\n",
    "    newListSAS = []\n",
    "    for word in words:\n",
    "        newListSAS.append(stemmer.stem(word))\n",
    "    return \"\".join(newListSAS)\n",
    "\n",
    "def calculate_perplexity(ngrams, n, test_set):\n",
    "    prod = 1\n",
    "    N = 0\n",
    "    for line in test_set:\n",
    "        split = line.split()\n",
    "        for k in range(n, len(split)):\n",
    "            suffix = split[k - 1]\n",
    "            word = split[k]\n",
    "            likelihood = eval_ngram_word(ngrams, suffix, word)\n",
    "            prod *= 1 / likelihood\n",
    "            N += 1\n",
    "    return abs(prod) ** (1.0 / N)\n",
    "\n",
    "# Make a n-gram out of list of strings, where each string is a sentance\n",
    "# Strings = array of strings\n",
    "# N = order of n-gram\n",
    "def make_ngram(strings, n=1):\n",
    "    gramDict = {}\n",
    "    # Loop strings\n",
    "    for string in strings:\n",
    "        split = string.split()\n",
    "        # Loop substrings of length n and append to dictionary\n",
    "        for k in range(0, len(split) - n + 1):\n",
    "            tmpWord  = \" \".join(split[k:k+n])\n",
    "            if tmpWord not in gramDict.keys():\n",
    "                gramDict[tmpWord] = 1\n",
    "            else:\n",
    "                gramDict[tmpWord] += 1\n",
    "    return gramDict\n",
    "\n",
    "# Evaluates given unigram as count(word) / count(*)\n",
    "def eval_unigram(ngram, word, alpha=1.0):\n",
    "    wordcount = sum(ngram.values())\n",
    "    if word in ngram.keys():\n",
    "        return alpha* ngram[word] / wordcount\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Evaluates given n-gram as: n-gram_count(string) / (n-1)-gram_count(string - 1)\n",
    "def eval_gram(ngram, mgram, suffix, word, k=0, alpha=1.0, wordcount = 1):\n",
    "    cond = suffix + ' ' + word\n",
    "\n",
    "    if cond in ngram.keys():\n",
    "        divident = ngram[cond]\n",
    "        divisor = mgram[suffix]\n",
    "        return (divident + k) / (divisor + k * wordcount)\n",
    "    else:\n",
    "        return k / wordcount\n",
    "\n",
    "def kneser_Ney(grams, suffix, word, d=0.5):\n",
    "    cond = suffix + ' ' + word\n",
    "    divident = 0\n",
    "    divisor = grams[1][suffix]\n",
    "    if cond in grams[0].keys():\n",
    "        divident = max(grams[0][cond] - d, 0)\n",
    "\n",
    "    norm_constant = d / divisor\n",
    "    return divident / divisor + norm_constant * eval_unigram(grams[1], word)\n",
    "\n",
    "# Evaluates n-gram with given smoothing parameters\n",
    "# n-grams: List of n-grams in decreasing order\n",
    "# word: word we are evaluating\n",
    "# suffix: string trailing the word\n",
    "def eval_ngram_word(ngrams, suffix, word, smoothing = \"\", k = 1, alpha=1.0):\n",
    "\n",
    "    # add k-smoothing\n",
    "    if smoothing == \"k\":\n",
    "        assert len(ngrams) >= 2, \"Atleast two grams yo\"\n",
    "\n",
    "        totalWords = sum(ngrams[len(ngrams) - 1].values())\n",
    "        return eval_gram(ngrams[0], ngrams[1], suffix, word, k=k, wordcount=totalWords)\n",
    "    # discount smoothing got lazy so like kneser-ney\n",
    "    if smoothing == \"d\":\n",
    "        return kneser_Ney(ngrams, suffix, word)\n",
    "    # BackOff smoothing\n",
    "    if smoothing == \"bo\":\n",
    "        if suffix + ' ' + word in ngrams[0].keys():\n",
    "            return alpha * eval_gram(ngrams[0], ngrams[1], suffix, word)\n",
    "        else:\n",
    "            if len(ngrams) > 1:\n",
    "                return eval_ngram_word(ngrams[1:], suffix, word, smoothing=smoothing)\n",
    "            else:\n",
    "                return eval_unigram(ngrams[0], word)\n",
    "    # Stupid BackOff\n",
    "    if smoothing == \"sbo\":\n",
    "        if suffix + ' ' + word in ngrams[0].keys():\n",
    "            return alpha * eval_gram(ngrams[0], ngrams[1], suffix, word)\n",
    "        else:\n",
    "            if len(ngrams) > 1:\n",
    "                return eval_ngram_word(ngrams[1:], suffix, word, smoothing = smoothing, k = k, alpha=(alpha*0.4))\n",
    "            else:\n",
    "                return eval_unigram(ngrams[0], word, alpha=alpha)\n",
    "    # Normal n-gram\n",
    "    else:\n",
    "        assert len(ngrams) >= 2, \"Atleast two grams yo\"\n",
    "\n",
    "        return eval_gram(ngrams[0], ngrams[1], suffix, word)\n",
    "\n",
    "def calculatePMI(ngram2, ngram1, word1, word2):\n",
    "    if word1 + ' ' + word2 in ngram2.keys():\n",
    "        divident = eval_gram(ngram2, ngram1, word1, word2)\n",
    "        divisor = eval_unigram(ngram1, word1) * eval_unigram(ngram1, word2)\n",
    "        return np.log2(divident / divisor)\n",
    "    return 0\n",
    "\n",
    "def find_ml_word(gram2, gram1, word):\n",
    "    keyList = []\n",
    "    for key in gram2.keys():\n",
    "        split = key.split()\n",
    "        if \" \".join(split[:-1]) == word:\n",
    "            keyList.append(key)\n",
    "\n",
    "    # Change index next to split according to the n-gram! TODO: make it bettar!\n",
    "    test = [(key.split()[2], eval_ngram_word([gram2, gram1], word, key.split()[2], smoothing=\"k\")) for key in keyList]\n",
    "    if len(test) > 0:\n",
    "        ml = max(test, key=itemgetter(1))[0]\n",
    "        ml_word = ml\n",
    "    else:\n",
    "        ml_word = '<s>'\n",
    "    return ml_word\n",
    "\n",
    "def make_a_story(grams, start_word = \"i\", story_length=100):\n",
    "    prevString = start_word\n",
    "    newcond = start_word\n",
    "    story = start_word.split()\n",
    "    for i in range(story_length):\n",
    "        prevString = find_ml_word(grams[0], grams[1], newcond)\n",
    "        story.append(prevString)\n",
    "        newcond = \" \".join(story[-2:])\n",
    "\n",
    "    return \" \".join(story)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Norton products ', ' neg ', 'but if i installed either one of these norton products neither works after installation']\n"
     ]
    }
   ],
   "source": [
    "# ========================\n",
    "# 4.8:\n",
    "# Write a program to compute unsmoothed unigrams and bigrams.\n",
    "# ========================\n",
    "\n",
    "path = \"Dataset2.txt\"\n",
    "Reviews = []\n",
    "ngramList = []\n",
    "c = Counter()\n",
    "with open(path) as f:\n",
    "    for line in f.readlines():\n",
    "        review = line.split('$')\n",
    "        tokenized = rTokenizer.tokenize(review[2].lower().strip())\n",
    "        stemmed = '<s> <s> ' + stemMyStrings(\" \".join(tokenized)) + ' </s>'\n",
    "\n",
    "        ngramList.append(stemmed)\n",
    "        review[2] = \" \".join(tokenized)\n",
    "        Reviews.append(review)\n",
    "\n",
    "# If word breaks are included, need que to start new sentances\n",
    "ngramList.append('</s> <s>')\n",
    "\n",
    "print(Reviews[1])\n",
    "trigram = make_ngram(ngramList, 3)\n",
    "bigram = make_ngram(ngramList, 2)\n",
    "unigram = make_ngram(ngramList, 1)\n",
    "grams = [trigram, bigram, unigram]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> but it s a great phone </s> <s> <s> the only\n"
     ]
    }
   ],
   "source": [
    "# 4.10\n",
    "# Add an option to your program to generate random sentences.\n",
    "# I played around with the sentance endings to escape loops .....\n",
    "print(make_a_story(grams, start_word=\"<s> but\", story_length=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0001641994644571313\n",
      "8.347749240903163\n"
     ]
    }
   ],
   "source": [
    "# 4.11\n",
    "# Add an option to your program to compute the perplexity of a test set. ---> calculate_perplexity() method\n",
    "# Evaluate my n-gram\n",
    "grams = [bigram, unigram]\n",
    "print(eval_ngram_word(grams, \"like\", \"to\", smoothing = \"k\"))\n",
    "\n",
    "# PMI calculation\n",
    "print(calculatePMI(bigram, unigram, \"i\", \"like\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
