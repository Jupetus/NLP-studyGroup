{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet_ic to\n",
      "[nltk_data]     C:\\Users\\Jukka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Jukka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw to\n",
      "[nltk_data]     C:\\Users\\Jukka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet_ic\n",
    "import numpy as np\n",
    "import nltk.tokenize\n",
    "from nltk.corpus import wordnet as wn\n",
    "import scipy.spatial.distance as distance\n",
    "import gensim\n",
    "\n",
    "\n",
    "nltk.download('wordnet_ic')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tutorial based on http://www.nltk.org/howto/wordnet.html\n",
    "\n",
    "## Words\n",
    "We can query words/lemmas with wn.synsets(\"word\")\n",
    "\n",
    "We can optionally pass arguments as <b>\"word.pos.nn\"</b> to query a specific lemma\n",
    "\n",
    "Also possible to translate meanings to different languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('cool.n.01'), Synset('aplomb.n.01'), Synset('cool.v.01'), Synset('cool.v.02'), Synset('cool.v.03'), Synset('cool.a.01'), Synset('cool.s.02'), Synset('cool.a.03'), Synset('cool.a.04'), Synset('cool.s.05'), Synset('cool.s.06')]\n",
      "============\n",
      "the quality of being at a refreshingly low temperature\n",
      "['cool']\n",
      "['viileys', 'vilpoisuus']\n",
      "============\n",
      "[Synset('coldness.n.02'), Synset('aplomb.n.01'), Synset('coldness.n.03'), Synset('chilliness.n.01'), Synset('cool.n.01'), Synset('distance.n.04'), Synset('withdrawal.n.04')]\n"
     ]
    }
   ],
   "source": [
    "# All meanings for word cool\n",
    "print(wn.synsets('cool'))\n",
    "# Most frequent meaning and translations\n",
    "print(\"============\")\n",
    "print(wn.synset('cool.n.01').definition())\n",
    "print(wn.synset('cool.n.01').lemma_names('eng'))\n",
    "print(wn.synset('cool.n.01').lemma_names('fin'))\n",
    "print(\"============\")\n",
    "# We can also map back to english!\n",
    "print(wn.synsets('viileys', lang='fin'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synsets\n",
    "\n",
    "A set of words that share a common meaning\n",
    "\n",
    "Each synset has one or more lemmas\n",
    "\n",
    "Lemmas in synset then contains \n",
    "* Gloss - dictionary like definition\n",
    "* Examples - Examples of word usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to explore word synsets\n",
    "# Input a word, optional language and Noun (n), Verb(v), Adj(a)\n",
    "def print_word_synsets(word, l='eng', p=\"\"):\n",
    "    print('Count of near synonyms: ' + str(len(wn.synsets(word, lang=l, pos=p))))\n",
    "    for syn in wn.synsets(word, lang=l, pos=p):\n",
    "        print(\"====================================================\")\n",
    "        print(\"Synset: \" + syn.name())\n",
    "        print(\"Gloss: \" + wn.synset(syn.name()).definition())\n",
    "        print(\"examples: \" + str(len(wn.synset(syn.name()).examples())))\n",
    "        for example in wn.synset(syn.name()).examples():\n",
    "            print(\"\\t\" + example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of near synonyms: 3\n",
      "====================================================\n",
      "Synset: frost.v.01\n",
      "Gloss: decorate with frosting\n",
      "examples: 1\n",
      "\tfrost a cake\n",
      "====================================================\n",
      "Synset: ice.v.02\n",
      "Gloss: cause to become ice or icy\n",
      "examples: 1\n",
      "\tan iced summer drink\n",
      "====================================================\n",
      "Synset: ice.v.03\n",
      "Gloss: put ice on or put on ice\n",
      "examples: 1\n",
      "\tIce your sprained limbs\n"
     ]
    }
   ],
   "source": [
    "print_word_synsets('ice', l='eng', p=\"v\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarities\n",
    "\n",
    "We can measure word similarities on wordnet\n",
    "\n",
    "In wordnet similarity means that a word can be replaced by another one always\n",
    "\n",
    "Word similarity can mean <b>similarity</b> or <b>words being related </b>\n",
    "* Similar <-> tractor, car\n",
    "* Related but not similar car <-> gasoline\n",
    "\n",
    "Words are defined to be similar if\n",
    "* they share meaning(s) \n",
    "* are near synonyms \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dog = wn.synset('dog.n.01')\n",
    "cat = wn.synset('cat.n.01')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Path similarity\n",
    "\n",
    "Path similarity is calculated by how many edges there are between two words\n",
    "\n",
    "for $pathlen(a,b)$ we pick the edge with least distance\n",
    "\n",
    "Every edge is defined to have the same weight\n",
    "\n",
    "$sim(a,b) = \\frac{1}{pathlen(a,b)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dog.path_similarity(cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information content\n",
    "\n",
    "IC tries to fix that every edge has the same weight\n",
    "\n",
    "We can use information content to determine similarities between words. Always based on probability of finding a word in a corpus\n",
    "\n",
    "In NLTK we can download ready made IC-dictionaries or use our own.\n",
    "\n",
    "\n",
    "## Resnik similarity\n",
    "\n",
    "Similarity based on LCS, that refers to lowest common subsumer. e.g. (word with lowest distance from two concepts)\n",
    "* cat, dog have hypernym of animal and mammal\n",
    "* mammal is lower common hypernym\n",
    "\n",
    "$P(c) =  \\frac{\\sum count(w)}{N}$\n",
    "\n",
    "$ LCS(a, b) = $ first node that's hypernym for $a,b$ \n",
    "\n",
    "$sim(a,b)_{resnik} = -log( p( LCS(a, b ) )$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.911666509036577"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load and use ic-libraries\n",
    "brown_ic = wordnet_ic.ic('ic-brown.dat')\n",
    "semcor_ic = wordnet_ic.ic('ic-semcor.dat')\n",
    "dog.res_similarity(cat, brown_ic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lin-similarity\n",
    "\n",
    "Another way to calculate similarities based on information contents (based on resniks method)\n",
    "\n",
    "$sim(a,b) = \\frac{2 log P(LCS(a,b))}{log P(a) + log P(b)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8768009843733973"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dog.lin_similarity(cat, brown_ic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Worthwile to mention\n",
    "\n",
    "* Myrphs -> we can try to lemmatize the word\n",
    "* Domains of a word "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "church\n",
      "[Synset('computer_science.n.01')]\n"
     ]
    }
   ],
   "source": [
    "print(wn.morphy('churches'))\n",
    "print(wn.synset('code.n.03').topic_domains())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesk algorithm\n",
    "\n",
    "Tries to decode a word meaning based on the surrounding words.\n",
    "\n",
    "We loop over all the senses of the word and choose one that shares most words with the target\n",
    "\n",
    "simplified lesk algorithm in the book didnt seem to include hyponyms, but it seems to work better with those (as we get more descriptions for a word)\n",
    "\n",
    "<b> Is similarity based on glosses </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate how many words in query sentance agree with examples\n",
    "def overlapcontext(sense, sentence):\n",
    "    sentence = nltk.word_tokenize(sentence)\n",
    "    gloss = set(nltk.word_tokenize(sense.definition()))\n",
    "    for expmle in sense.examples():\n",
    "         gloss.union(nltk.word_tokenize(expmle))\n",
    "    return len(gloss.intersection(sentence))\n",
    "\n",
    "# Compare senses of a word and pick one that has most overlapping words\n",
    "def lesk(word, sentence):\n",
    "    bestsense = None\n",
    "    maxoverlap = 0\n",
    "    # Try to lemmatize the word with morphy\n",
    "    word = wn.morphy(word) if wn.morphy(word) is not None else word\n",
    "    # Loop all the possible meanings of the word\n",
    "    for sense in wn.synsets(word):\n",
    "        overlap = overlapcontext(sense, sentence)\n",
    "        # Subtypes might be related to word!\n",
    "        for h in sense.hyponyms():\n",
    "            overlap = overlap + overlapcontext(h, sentence)\n",
    "        if overlap > maxoverlap:\n",
    "                maxoverlap = overlap\n",
    "                bestsense = sense\n",
    "    return bestsense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('cone.n.03') | cone-shaped mass of ovule- or spore-bearing scales or bracts\n",
      "[Synset('fir_cone.n.01'), Synset('galbulus.n.01'), Synset('pinecone.n.01')]\n"
     ]
    }
   ],
   "source": [
    "sense = lesk(\"cone\", \"A traffic cone was tipped over\")\n",
    "print(str(sense) + \" | \" + str(sense.definition()))\n",
    "print(str(sense.hyponyms()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesk algorithm with W2V similarity measure\n",
    "\n",
    "Instead of counting the overlap, we can pick the meaning that is most similar to the query sentance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load google's w2v\n",
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format('./model/GoogleNews-vectors-negative300.bin', binary=True, limit=200000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate w2v cosine sim between two words\n",
    "def calculate_w2v_cossim(sentance1, sentance2):\n",
    "    doc1 = [word2vec.wv[word] for word in sentance1 if word in word2vec.wv]\n",
    "    doc2 = [word2vec.wv[word] for word in sentance2 if word in word2vec.wv]\n",
    "    \n",
    "    doc1 = np.mean(doc1, axis=0)\n",
    "    doc2 = np.mean(doc2, axis=0)\n",
    "    return 1 - distance.cosine(doc1, doc2)\n",
    "\n",
    "# loop over all examples of a sense and pick the most similar one\n",
    "def overlap_w2v_context(sense, sentence):\n",
    "    sentence = nltk.word_tokenize(sentence)\n",
    "    gloss = nltk.word_tokenize(sense.definition())\n",
    "    \n",
    "    similarities = []\n",
    "    similarities.append(calculate_w2v_cossim(sentence, gloss))\n",
    "    for expmle in sense.examples():\n",
    "        similarities.append(calculate_w2v_cossim(sentence, nltk.word_tokenize(expmle)))\n",
    "    return np.max(similarities)\n",
    "    \n",
    "# w2v - Lesk algorithm!\n",
    "def lesk_w2v(word, sentence):\n",
    "    bestsense = None\n",
    "    maxsimilarity = 0\n",
    "    word = wn.morphy(word) if wn.morphy(word) is not None else word\n",
    "    # Loop all the possible meanings of the word\n",
    "    for sense in wn.synsets(word):\n",
    "        tmp = []\n",
    "        tmp.append(overlap_w2v_context(sense, sentence))\n",
    "        for h in sense.hyponyms():\n",
    "            tmp.append(overlapcontext(h, sentence)) \n",
    "        # Only pick most similar entry for use, so we dont favor meanings with many examples\n",
    "        similarity = np.max(tmp)\n",
    "        if similarity > maxsimilarity:\n",
    "                maxsimilarity = similarity\n",
    "                bestsense = sense\n",
    "    return bestsense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('cook.n.01')\n",
      "someone who cooks food\n",
      "========================\n",
      "Synset('cook.v.03')\n",
      "transform and make suitable for consumption by heating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python34\\lib\\site-packages\\ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "c:\\python34\\lib\\site-packages\\ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "sense = lesk_w2v(\"cook\", \"i like to cook food\")\n",
    "sense2 = lesk(\"cook\", \"i like to cook food\")\n",
    "print(sense)\n",
    "print(sense.definition())\n",
    "print(\"========================\")\n",
    "print(sense2)\n",
    "print(sense2.definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
