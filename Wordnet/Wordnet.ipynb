{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python34\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet_ic to\n",
      "[nltk_data]     C:\\Users\\Jukka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Jukka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw to\n",
      "[nltk_data]     C:\\Users\\Jukka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet_ic\n",
    "import numpy as np\n",
    "import nltk.tokenize\n",
    "from nltk.corpus import wordnet as wn\n",
    "import scipy.spatial.distance as distance\n",
    "import gensim\n",
    "\n",
    "# Information content for similarities\n",
    "nltk.download('wordnet_ic')\n",
    "# Wordnet\n",
    "nltk.download('wordnet')\n",
    "# Multilingual wordnet\n",
    "nltk.download('omw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tutorial based on http://www.nltk.org/howto/wordnet.html\n",
    "\n",
    "## Words\n",
    "We can query words/lemmas with wn.synsets(\"word\")\n",
    "\n",
    "We can optionally pass arguments as <b>\"word.pos.nn\"</b> to query a specific lemma\n",
    "\n",
    "Also possible to translate meanings to different languages\n",
    "\n",
    "Can also use \n",
    "\n",
    "wn.synsets('dog', pos=wn.VERB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('cool.n.01'), Synset('aplomb.n.01'), Synset('cool.v.01'), Synset('cool.v.02'), Synset('cool.v.03'), Synset('cool.a.01'), Synset('cool.s.02'), Synset('cool.a.03'), Synset('cool.a.04'), Synset('cool.s.05'), Synset('cool.s.06')]\n",
      "============\n",
      "the quality of being at a refreshingly low temperature\n",
      "['cool']\n",
      "['viileys', 'vilpoisuus']\n",
      "============\n",
      "[Synset('coldness.n.02'), Synset('aplomb.n.01'), Synset('coldness.n.03'), Synset('chilliness.n.01'), Synset('cool.n.01'), Synset('distance.n.04'), Synset('withdrawal.n.04')]\n"
     ]
    }
   ],
   "source": [
    "# All meanings for word cool\n",
    "print(wn.synsets('cool'))\n",
    "# Most frequent meaning and translations\n",
    "print(\"============\")\n",
    "print(wn.synset('cool.n.01').definition())\n",
    "print(wn.synset('cool.n.01').lemma_names('eng'))\n",
    "print(wn.synset('cool.n.01').lemma_names('fin'))\n",
    "print(\"============\")\n",
    "# We can also map back to english!\n",
    "print(wn.synsets('viileys', lang='fin'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synsets\n",
    "\n",
    "A set of words that share a common meaning\n",
    "\n",
    "Each synset has one or more lemmas\n",
    "\n",
    "Lemmas in synset then contains \n",
    "* Gloss - dictionary like definition\n",
    "* Examples - Examples of word usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to explore word synsets\n",
    "# Input a word, optional language and Noun (n), Verb(v), Adj(a)\n",
    "def print_word_synsets(word, l='eng', p=\"\"):\n",
    "    print('Count of near synonyms: ' + str(len(wn.synsets(word, lang=l, pos=p))))\n",
    "    for syn in wn.synsets(word, lang=l, pos=p):\n",
    "        print(\"====================================================\")\n",
    "        print(\"Synset: \" + syn.name())\n",
    "        print(\"Gloss: \" + wn.synset(syn.name()).definition())\n",
    "        print(\"examples: \" + str(len(wn.synset(syn.name()).examples())))\n",
    "        for example in wn.synset(syn.name()).examples():\n",
    "            print(\"\\t\" + example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of near synonyms: 3\n",
      "====================================================\n",
      "Synset: frost.v.01\n",
      "Gloss: decorate with frosting\n",
      "examples: 1\n",
      "\tfrost a cake\n",
      "====================================================\n",
      "Synset: ice.v.02\n",
      "Gloss: cause to become ice or icy\n",
      "examples: 1\n",
      "\tan iced summer drink\n",
      "====================================================\n",
      "Synset: ice.v.03\n",
      "Gloss: put ice on or put on ice\n",
      "examples: 1\n",
      "\tIce your sprained limbs\n"
     ]
    }
   ],
   "source": [
    "print_word_synsets('ice', l='eng', p=\"v\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wordnet structure, relations\n",
    "\n",
    "We can travel the defined relations in the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemma: \tSynset('pizza.n.01')\n",
      "\n",
      "hypernyms: \t[Synset('dish.n.02')]\n",
      "hyponyms: \t[Synset('anchovy_pizza.n.01'), Synset('cheese_pizza.n.01'), Synset('pepperoni_pizza.n.01'), Synset('sausage_pizza.n.01'), Synset('sicilian_pizza.n.01')]\n",
      "member_mero: \t[]\n",
      "member_holo: \t[]\n",
      "Antonyms: \t[]\n",
      "\n",
      "\n",
      "Lemma: \tSynset('dive.v.01')\n",
      "\n",
      "hypernyms: \t[Synset('descend.v.01')]\n",
      "Troponym: \t[Synset('chute.v.01'), Synset('crash-dive.v.01'), Synset('duck.v.02'), Synset('nosedive.v.01'), Synset('power-dive.v.01')]\n",
      "Entails: \t[]\n",
      "Antonyms: \t[]\n"
     ]
    }
   ],
   "source": [
    "def print_noun_relations(synset):\n",
    "    print(\"Lemma: \\t\" + str(synset) + \"\\n\")\n",
    "    print(\"hypernyms: \\t\"+ str(synset.hypernyms()))\n",
    "    print(\"hyponyms: \\t\"+str(synset.hyponyms()))\n",
    "    print(\"member_mero: \\t\"+str(synset.member_meronyms()))\n",
    "    print(\"member_holo: \\t\"+str(synset.member_holonyms()))\n",
    "    print(\"Antonyms: \\t\"+str(synset.lemmas()[0].antonyms()))\n",
    "\n",
    "def print_verb_relations(synset):\n",
    "    print(\"Lemma: \\t\" + str(synset) + \"\\n\")\n",
    "    print(\"hypernyms: \\t\"+ str(synset.hypernyms()))\n",
    "    print(\"Troponym: \\t\"+str(synset.hyponyms()))\n",
    "    print(\"Entails: \\t\"+str(synset.entailments()))\n",
    "    print(\"Antonyms: \\t\"+str(synset.lemmas()[0].antonyms()))\n",
    "    \n",
    "# Travel the relations\n",
    "pizza = wn.synset(\"pizza.n.01\")\n",
    "print_noun_relations(pizza)\n",
    "print(\"\\n\")\n",
    "diving = wn.synset(\"dive.v.01\")\n",
    "print_verb_relations(diving)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarities\n",
    "\n",
    "We can measure word similarities on wordnet\n",
    "\n",
    "In wordnet similarity means that a word can be replaced by another one always\n",
    "\n",
    "Word similarity can mean <b>similarity</b> or <b>words being related </b>\n",
    "* Similar <-> tractor, car\n",
    "* Related but not similar car <-> gasoline\n",
    "\n",
    "Words are defined to be similar if\n",
    "* they share meaning(s) \n",
    "* are near synonyms \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dog = wn.synset('dog.n.01')\n",
    "cat = wn.synset('cat.n.01')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Path similarity\n",
    "\n",
    "Path similarity is calculated by how many edges there are between two words\n",
    "\n",
    "for $pathlen(a,b)$ we pick the edge with least distance\n",
    "\n",
    "Every edge is defined to have the same weight\n",
    "\n",
    "$sim(a,b) = \\frac{1}{pathlen(a,b)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dog.path_similarity(cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information content\n",
    "\n",
    "IC tries to fix that every edge has the same weight\n",
    "\n",
    "We can use information content to determine similarities between words. Always based on probability of finding a word in a corpus\n",
    "\n",
    "In NLTK we can download ready made IC-dictionaries or use our own.\n",
    "\n",
    "\n",
    "## Resnik similarity\n",
    "\n",
    "Similarity based on LCS, that refers to lowest common subsumer. e.g. (word with lowest distance from two concepts)\n",
    "* cat, dog have hypernym of animal and mammal\n",
    "* mammal is lower common hypernym\n",
    "\n",
    "$P(c) =  \\frac{\\sum count(w)}{N}$\n",
    "\n",
    "$ LCS(a, b) = $ first node that's hypernym for $a,b$ \n",
    "\n",
    "$sim(a,b)_{resnik} = -log( P( LCS(a, b ) )$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.911666509036577\n",
      "7.2549003421277245\n"
     ]
    }
   ],
   "source": [
    "# Load and use ic-libraries\n",
    "brown_ic = wordnet_ic.ic('ic-brown.dat')\n",
    "semcor_ic = wordnet_ic.ic('ic-semcor.dat')\n",
    "print(dog.res_similarity(cat, brown_ic))\n",
    "print(dog.res_similarity(cat, semcor_ic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lin-similarity\n",
    "\n",
    "Another way to calculate similarities based on information contents (based on resniks method)\n",
    "\n",
    "$sim(a,b) = \\frac{2 log P(LCS(a,b))}{log P(a) + log P(b)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8768009843733973\n",
      "0.8863288628086228\n"
     ]
    }
   ],
   "source": [
    "print(dog.lin_similarity(cat, brown_ic))\n",
    "print(dog.lin_similarity(cat, semcor_ic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Worthwile to mention\n",
    "\n",
    "* Myrphs -> we can try to lemmatize the word\n",
    "* Domains of a word "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "church\n",
      "[Synset('computer_science.n.01')]\n"
     ]
    }
   ],
   "source": [
    "# Extras\n",
    "print(wn.morphy('churches'))\n",
    "print(wn.synset('code.n.03').topic_domains())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesk algorithm\n",
    "\n",
    "Tries to decode a word meaning based on the surrounding words.\n",
    "\n",
    "We loop over all the senses of the word and choose one that shares most words with the target\n",
    "\n",
    "simplified lesk algorithm in the book didnt seem to include hyponyms, but it seems to work better with those (as we get more descriptions for a word)\n",
    "\n",
    "<b> Is similarity based on glosses </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate how many words in query sentance agree with examples\n",
    "def overlapcontext(sense, sentence):\n",
    "    sentence = nltk.word_tokenize(sentence)\n",
    "    gloss = set(nltk.word_tokenize(sense.definition()))\n",
    "    for expmle in sense.examples():\n",
    "         gloss.union(nltk.word_tokenize(expmle))\n",
    "    return len(gloss.intersection(sentence))\n",
    "\n",
    "# Compare senses of a word and pick one that has most overlapping words\n",
    "def lesk(word, sentence):\n",
    "    # Would be also fine to assign most frequent sense here\n",
    "    bestsense = None\n",
    "    maxoverlap = 0\n",
    "    # Try to lemmatize the word with morphy\n",
    "    word = wn.morphy(word) if wn.morphy(word) is not None else word\n",
    "    # Loop all the possible meanings of the word\n",
    "    for sense in wn.synsets(word):\n",
    "        overlap = overlapcontext(sense, sentence)\n",
    "        # Subtypes might be related to word!\n",
    "        for h in sense.hyponyms():\n",
    "            overlap = overlap + overlapcontext(h, sentence)\n",
    "        if overlap > maxoverlap:\n",
    "                maxoverlap = overlap\n",
    "                bestsense = sense\n",
    "    return bestsense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('cone.n.03') | cone-shaped mass of ovule- or spore-bearing scales or bracts\n",
      "[Synset('fir_cone.n.01'), Synset('galbulus.n.01'), Synset('pinecone.n.01')]\n"
     ]
    }
   ],
   "source": [
    "sense = lesk(\"cone\", \"A traffic cone was tipped over\")\n",
    "print(str(sense) + \" | \" + str(sense.definition()))\n",
    "print(str(sense.hyponyms()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesk algorithm with W2V similarity measure\n",
    "\n",
    "Instead of counting the overlap, we can pick the meaning that is most similar to the query sentance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python34\\lib\\site-packages\\ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# Load google's w2v\n",
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format('./model/GoogleNews-vectors-negative300.bin', binary=True, limit=200000) \n",
    "word_vectors = word2vec.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate w2v cosine sim between two words\n",
    "def calculate_w2v_cossim(sentance1, sentance2):\n",
    "    doc1 = [word_vectors[word] for word in sentance1 if word in word2vec.vocab]\n",
    "    doc2 = [word_vectors[word] for word in sentance2 if word in word2vec.vocab]\n",
    "    \n",
    "    doc1 = np.mean(doc1, axis=0)\n",
    "    doc2 = np.mean(doc2, axis=0)\n",
    "    return 1 - distance.cosine(doc1, doc2)\n",
    "\n",
    "# loop over all examples of a sense and pick the most similar one\n",
    "def overlap_w2v_context(sense, sentence):\n",
    "    sentence = nltk.word_tokenize(sentence)\n",
    "    gloss = nltk.word_tokenize(sense.definition())\n",
    "    \n",
    "    similarities = []\n",
    "    similarities.append(calculate_w2v_cossim(sentence, gloss))\n",
    "    for expmle in sense.examples():\n",
    "        similarities.append(calculate_w2v_cossim(sentence, nltk.word_tokenize(expmle)))\n",
    "    return np.max(similarities)\n",
    "    \n",
    "# w2v - Lesk algorithm!\n",
    "def lesk_w2v(word, sentence):\n",
    "    bestsense = None\n",
    "    maxsimilarity = 0\n",
    "    word = wn.morphy(word) if wn.morphy(word) is not None else word\n",
    "    # Loop all the possible meanings of the word\n",
    "    for sense in wn.synsets(word):\n",
    "        tmp = []\n",
    "        tmp.append(overlap_w2v_context(sense, sentence))\n",
    "        for h in sense.hyponyms():\n",
    "            tmp.append(overlap_w2v_context(h, sentence)) \n",
    "        # Only pick most similar entry for use, so we dont favor meanings with many examples\n",
    "        similarity = np.max(tmp)\n",
    "        if similarity > maxsimilarity:\n",
    "                maxsimilarity = similarity\n",
    "                bestsense = sense\n",
    "    return bestsense\n",
    "\n",
    "# Get outcomes for traditional and W2V lesk algorithms\n",
    "def test_lesks(word, sentance):\n",
    "    sense_w2v = lesk_w2v(word, sentance)\n",
    "    sense_trad = lesk(word, sentance)\n",
    "    \n",
    "    print(\"======== Traditional =======\")\n",
    "    print(sense_trad)\n",
    "    print(sense_trad.definition())\n",
    "    print(\"=========== V2W ============\")\n",
    "    print(sense_w2v)\n",
    "    print(sense_w2v.definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Traditional =======\n",
      "Synset('cook.v.03')\n",
      "transform and make suitable for consumption by heating\n",
      "=========== V2W ============\n",
      "Synset('cook.n.01')\n",
      "someone who cooks food\n"
     ]
    }
   ],
   "source": [
    "test_lesks(\"cook\", \"i like to cook food\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Traditional =======\n",
      "Synset('cone.n.03')\n",
      "cone-shaped mass of ovule- or spore-bearing scales or bracts\n",
      "=========== V2W ============\n",
      "Synset('cone.n.03')\n",
      "cone-shaped mass of ovule- or spore-bearing scales or bracts\n"
     ]
    }
   ],
   "source": [
    "test_lesks(\"cone\", \"there are many pine cones in the trees\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python34\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "c:\\python34\\lib\\site-packages\\numpy\\core\\_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "c:\\python34\\lib\\site-packages\\numpy\\core\\_methods.py:26: RuntimeWarning: invalid value encountered in reduce\n",
      "  return umr_maximum(a, axis, None, out, keepdims)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traditional algorithm got right: 0.376817981489643\n",
      "\n",
      "\n",
      "W2V algorithm got right: 0.35654473336271486\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "# Read and prep XML\n",
    "tree = ET.parse(\"eng-coarse-all-words.xml\")\n",
    "root = tree.getroot()\n",
    "texts = root.findall('text')\n",
    "keyDictionary = {}\n",
    "\n",
    "# read the keys!\n",
    "with open(\"fs_baseline.key.txt\") as f:\n",
    "    for line in f.readlines():\n",
    "        tmp = line.split()\n",
    "        # 1: id present with the data, 2: Key of the item in wordnet\n",
    "        keyDictionary[tmp[1]] = tmp[2]\n",
    "\n",
    "right_traditional = 0\n",
    "right_w2v = 0\n",
    "counter = 0\n",
    "# Loop over the text and instances\n",
    "for text in texts:\n",
    "    # One text contains many sentace - elements\n",
    "    for sentance in text.findall('sentence'):\n",
    "        text = \" \".join(sentance.itertext()).replace('\\n','')\n",
    "        # We want to find meaning for words behind <instance> tags\n",
    "        for instance in sentance.findall('instance'):\n",
    "            instance_id = instance.get('id')\n",
    "            lemma_traditional = lesk(instance.text, text)\n",
    "            lemma_w2v = lesk_w2v(instance.text, text)\n",
    "            # See if we had returned a meaning, and if the keys matched\n",
    "            if lemma_traditional:\n",
    "                # All keys distinct and im lazy -> this should be fine\n",
    "                for lemma in lemma_traditional.lemmas():\n",
    "                    if lemma.key() == keyDictionary[instance_id]:\n",
    "                        right_traditional += 1\n",
    "\n",
    "            if lemma_w2v:\n",
    "                for lemma in lemma_w2v.lemmas():\n",
    "                    if lemma.key() == keyDictionary[instance_id]:\n",
    "                        right_w2v += 1\n",
    "    \n",
    "print(\"Traditional algorithm got right: \" + str(right_traditional/len(keyDictionary)))\n",
    "print(\"\\n\")\n",
    "print(\"W2V algorithm got right: \" + str(right_w2v/len(keyDictionary)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
