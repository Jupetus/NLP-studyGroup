{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===============================\n",
    "# Translation from pure python to notebook for easier read!\n",
    "#===============================\n",
    "import numpy as np\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================\n",
    "# HW - Predict the centiment of the product with Logistic Regression! \n",
    "# (Using pytorch!)\n",
    "# \n",
    "# Using product review dataset for sentiment analysis:\n",
    "# http://people.mpi-inf.mpg.de/~smukherjee/data/\n",
    "#\n",
    "# Based on the example:\n",
    "# http://pytorch.org/tutorials/beginner/nlp/deep_learning_tutorial.html#sphx-glr-beginner-nlp-deep-learning-tutorial-py)\n",
    "# ===================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_dim, output_dim):\n",
    "    # We don't need the softmax layer here since CrossEntropyLoss already\n",
    "    # uses it internally.\n",
    "    model = torch.nn.Sequential()\n",
    "    model.add_module(\"linear\", torch.nn.Linear(input_dim, output_dim, bias=False))\n",
    "    # Output -> sigmoid \n",
    "    model.add_module(\"output\", torch.nn.Sigmoid())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loss, optimizer, x_val, y_val):\n",
    "    x = Variable(x_val, requires_grad=False)\n",
    "    y = Variable(y_val, requires_grad=False)\n",
    "    # Reset gradient\n",
    "    optimizer.zero_grad()\n",
    "    # Forward\n",
    "    fx = model.forward(x)\n",
    "    output = loss.forward(fx, y)\n",
    "    # Backward\n",
    "    output.backward()\n",
    "    # Update parameters\n",
    "    optimizer.step()\n",
    "    return output.data[0]\n",
    "\n",
    "def predict(model, x_val):\n",
    "    x = Variable(x_val, requires_grad=False)\n",
    "    output = model.forward(x)\n",
    "    return output.data.numpy().argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data and stem it\n",
    "# This part could been done better, eh!\n",
    "\n",
    "def stemMyStrings(words):\n",
    "    stemmer = PorterStemmer()  # SnowballStemmer('english')\n",
    "    newListSAS = []\n",
    "    for word in words:\n",
    "        newListSAS.append(stemmer.stem(word))\n",
    "    return \"\".join(newListSAS)\n",
    "\n",
    "def load_data():\n",
    "    rTokenizer = RegexpTokenizer('\\w+')\n",
    "    path = \"Dataset2.txt\"\n",
    "    Reviews = []\n",
    "    # Read review and return tuples of ('Comment', 'sentiment = pos | neg')\n",
    "    with open(path) as f:\n",
    "        for line in f.readlines():\n",
    "            review = line.split('$')\n",
    "            tokenized = rTokenizer.tokenize(review[2].lower().strip())\n",
    "            stemmed = stemMyStrings(\" \".join(tokenized))\n",
    "\n",
    "            review[2] = \" \".join(tokenized)\n",
    "            Reviews.append((stemmed, review[1]))\n",
    "    return Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn a word into BOW -presentation\n",
    "def make_bow_vector(sentence, word_to_ix):\n",
    "    vec = np.zeros(len(word_to_ix))\n",
    "    for word in sentence:\n",
    "        vec[word_to_ix[word]] += 1\n",
    "    return vec\n",
    "\n",
    "# Target ID of positive or negative comment\n",
    "def make_target(label):\n",
    "    label_to_ix = {\"neg\": 0, \"pos\": 1}\n",
    "    return label_to_ix[label.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20, cost = 3.487330, acc = 71.15%\n",
      "Epoch 40, cost = 3.303030, acc = 73.24%\n",
      "Epoch 60, cost = 3.194058, acc = 74.41%\n",
      "Epoch 80, cost = 3.116109, acc = 75.85%\n",
      "Epoch 100, cost = 3.054919, acc = 76.11%\n",
      "Epoch 120, cost = 3.004274, acc = 76.50%\n",
      "Epoch 140, cost = 2.961023, acc = 76.24%\n",
      "Epoch 160, cost = 2.923334, acc = 76.63%\n",
      "Epoch 180, cost = 2.889997, acc = 76.63%\n",
      "Epoch 200, cost = 2.860154, acc = 76.37%\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    torch.manual_seed(42)\n",
    "    all_data = load_data()\n",
    "    # Split data to training and testing!\n",
    "    offset = int(0.8 * len(all_data))\n",
    "    train_data = [(all_data[tmp][0].split(), all_data[tmp][1]) for tmp in range(0, offset)]\n",
    "    test_data =  [(all_data[tmp][0].split(), all_data[tmp][1]) for tmp in range(offset + 1, len(all_data))]\n",
    "    \n",
    "    # Make a mapping from word to a number! (BOW-presentation)\n",
    "    word_to_ix = {}\n",
    "    for sent, _ in train_data + test_data:\n",
    "        for word in sent:\n",
    "            if word not in word_to_ix:\n",
    "                word_to_ix[word] = len(word_to_ix)\n",
    "    # Words are then features\n",
    "    n_features = len(word_to_ix)\n",
    "    n_classes = 2\n",
    "    \n",
    "    # Make sentances to vectors from (BOW) - initialize torch vectors\n",
    "    train_X = np.array([make_bow_vector(data[0], word_to_ix) for data in train_data])\n",
    "    test_X = np.array([make_bow_vector(data[0], word_to_ix) for data in test_data])\n",
    "    train_Y = np.array([make_target(data[1]) for data in train_data])\n",
    "    test_Y = np.array([make_target(data[1]) for data in test_data])\n",
    "    train_X = torch.from_numpy(train_X).float()\n",
    "    test_X = torch.from_numpy(test_X).float()\n",
    "    train_Y = torch.from_numpy(train_Y).long()\n",
    "    \n",
    "    # Make model\n",
    "    model = build_model(n_features, n_classes)\n",
    "    loss = torch.nn.CrossEntropyLoss(size_average=True)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.025, momentum=0.9)\n",
    "    batch_size = 50\n",
    "    num_batches = len(train_data) // batch_size\n",
    "    # Loop num of epochs and batches\n",
    "    for epoch in range(200):\n",
    "        cost = 0.\n",
    "        for k in range(num_batches):\n",
    "            start, end = k * batch_size, (k + 1) * batch_size\n",
    "            cost += train(model, loss, optimizer, train_X[start:end], train_Y[start:end])\n",
    "            \n",
    "        # Test\n",
    "        pred_Y = predict(model, test_X)\n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            print(\"Epoch %d, cost = %f, acc = %.2f%%\" % (epoch + 1, cost / 10, 100. * np.mean(pred_Y == test_Y)))\n",
    "        \n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
